# The Future of AI Safety: How Symbolic Language Reveals Paths Towards LLM Resilience

### Introduction: The LLM Security Challenge

Within the dynamic landscape of technology, artificial intelligence, particularly Large Language Models (LLMs), has opened unprecedented avenues and opportunities. However, with this innovation comes a critical need: to ensure the security and reliability of these systems.

It is in this context that the role of **AI Red Teaming** becomes fundamental. Its mission is to go beyond superficial testing to ensure responsible use of models, mitigating risks and promoting their secure deployment.

As LLMs evolve, so does the complexity of attacks. Their inherent **plasticity and the vast, almost infinite, range of processes and responses** they can generate, present a unique challenge for security. Therefore, *red teaming* exercises can no longer rely solely on conventional methods; they now require an urgent call for **creativity and innovation** to uncover their most elusive vulnerabilities.

![AI Cognitive Overload](images/Cognitive.png)

---

### The Challenge of Conventional Approaches

With the rapid development of LLMs, security teams have worked tirelessly to mitigate known vulnerabilities. We have witnessed how many **jailbreak prompts and prompt injection techniques have been identified, filtered, and patched** to create a safer environment. However, this evolution has also revealed a fundamental truth: LLM security does not solely reside in the words we 'tell' them, but in **how the model internally processes information and contextual interaction.** Herein lies a vast ground for discovering new and creative vulnerabilities.

---

### Unveiling Vulnerabilities with Adversarial Symbolic Language

It was precisely this understanding that led me to explore an unconventional path. Inspired by a **structured symbolic language I practiced since childhood**, I decided to apply its rules and principles to interaction with LLMs. The hypothesis was ambitious: could this abstract approach induce **cognitive overload** in the model, leveraging the complexity of its internal processing and the potential lack of specific training data for such structures?

This language was built on transformation principles, where **certain linguistic elements were systematically converted into non-standard representations**, such as the substitution of vowels with a tiered numerical system. The addition of **special symbols acted as operators that drastically inverted or altered** the logic of these transformations. Furthermore, rules for **reordering or inversion of word sequences** were explored, adding another layer of abstraction and syntactic complexity. The intention was clear: not to trick the model with forbidden words, but to disorient its internal processing at a more fundamental level, beneath superficial semantics.

This research was based on several key theories:

* **Cognitive Overload:** The premise that the complexity and novelty of a symbolic language could saturate or disorient the LLM's processing mechanisms, exposing flaws in its alignment or exception handling.
* **Response Time Impact:** The observation that the speed or latency in the LLM's responses could be an indicator or even a vector for exploiting security breaches, affecting the veracity or coherence of its outputs.
* **Adversarial Social Engineering:** The possibility that, by provoking an error or inconsistency through symbolic language, one could 'appeal' to the LLM's 'logic' to manipulate its responses and create a contextual security breach, similar to a social engineering interaction.

---

### Discovery Methodology: Disrupting LLM Defenses

To test the symbolic language hypothesis, I developed a rigorous methodology combining innovation in prompt creation with systematic analysis. My tests were implemented using **Python 3** and managed via **Visual Studio Code**, which allowed for efficient management and **automation of scripts for large-scale testing** on publicly available models, including **Llama 3.2**. This approach facilitated efficient and repeatable exploration of adversarial interactions.

The core
